{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "TCS_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hYzW4ZG-Zwp"
      },
      "source": [
        "# **Traffic Control System**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhde1biSjajW"
      },
      "source": [
        "##Setup for TCS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yFRP04nQ78I",
        "outputId": "962b6317-8b77-413c-df0e-df77cb3d090f"
      },
      "source": [
        "!add-apt-repository ppa:sumo/stable -y\n",
        "!apt-get update -y\n",
        "!apt-get install sumo sumo-tools sumo-doc\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/sumo/stable/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Fetched 252 kB in 2s (157 kB/s)\n",
            "Reading package lists... Done\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/sumo/stable/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Fetched 252 kB in 2s (154 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "sumo is already the newest version (1.8.0+dfsg1-1).\n",
            "sumo-doc is already the newest version (1.8.0+dfsg1-1).\n",
            "sumo-tools is already the newest version (1.8.0+dfsg1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4GDWeoeQ71W",
        "outputId": "8d7946ea-3e80-4c7d-f27b-330ee6bc4848"
      },
      "source": [
        "!pip install traci"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: traci in /usr/local/lib/python3.6/dist-packages (1.8.0)\n",
            "Requirement already satisfied: sumolib>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from traci) (1.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D82HOcIVQ7r2"
      },
      "source": [
        "import os\n",
        "os.environ['SUMO_HOME'] = \"/usr/share/sumo/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDtBWEHAQdRr"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "  \n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'  # kill warning about tensorflow\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "#Model \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "#Simulation\n",
        "\n",
        "import traci\n",
        "import timeit\n",
        "\n",
        "# Training\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "from shutil import copyfile\n",
        "\n",
        "#Visualization\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#New \n",
        "import numpy as np\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0WB24KYRZPH"
      },
      "source": [
        "class TrafficGenerator:\n",
        "    def __init__(self, max_steps, n_cars_generated):\n",
        "        self._n_cars_generated = n_cars_generated  # how many cars per episode\n",
        "        self._max_steps = max_steps\n",
        "\n",
        "    def generate_routefile(self, seed):\n",
        "        \"\"\"\n",
        "        Generation of the route of every car for one episode\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)  # make tests reproducible\n",
        "\n",
        "        # the generation of cars is distributed according to a weibull distribution\n",
        "        timings = np.random.weibull(2, self._n_cars_generated)\n",
        "        timings = np.sort(timings)\n",
        "\n",
        "        # reshape the distribution to fit the interval 0:max_steps\n",
        "        car_gen_steps = []\n",
        "        min_old = math.floor(timings[1])\n",
        "        max_old = math.ceil(timings[-1])\n",
        "        min_new = 0\n",
        "        max_new = self._max_steps\n",
        "        for value in timings:\n",
        "            car_gen_steps = np.append(car_gen_steps, ((max_new - min_new) / (max_old - min_old)) * (value - max_old) + max_new)\n",
        "\n",
        "        car_gen_steps = np.rint(car_gen_steps)  # round every value to int -> effective steps when a car will be generated\n",
        "\n",
        "        # produce the file for cars generation, one car per line\n",
        "        with open(\"/content/episode_routes.rou.xml\", \"w\") as routes:\n",
        "            print(\"\"\"<routes>\n",
        "            <vType accel=\"1.0\" decel=\"4.5\" id=\"standard_car\" length=\"5.0\" minGap=\"2.5\" maxSpeed=\"25\" sigma=\"0.5\" />\n",
        "            <route id=\"W_N\" edges=\"W2TL TL2N\"/>\n",
        "            <route id=\"W_E\" edges=\"W2TL TL2E\"/>\n",
        "            <route id=\"W_S\" edges=\"W2TL TL2S\"/>\n",
        "            <route id=\"N_W\" edges=\"N2TL TL2W\"/>\n",
        "            <route id=\"N_E\" edges=\"N2TL TL2E\"/>\n",
        "            <route id=\"N_S\" edges=\"N2TL TL2S\"/>\n",
        "            <route id=\"E_W\" edges=\"E2TL TL2W\"/>\n",
        "            <route id=\"E_N\" edges=\"E2TL TL2N\"/>\n",
        "            <route id=\"E_S\" edges=\"E2TL TL2S\"/>\n",
        "            <route id=\"S_W\" edges=\"S2TL TL2W\"/>\n",
        "            <route id=\"S_N\" edges=\"S2TL TL2N\"/>\n",
        "            <route id=\"S_E\" edges=\"S2TL TL2E\"/>\"\"\", file=routes)\n",
        "\n",
        "            for car_counter, step in enumerate(car_gen_steps):\n",
        "                straight_or_turn = np.random.uniform()\n",
        "                if straight_or_turn < 0.75:  # choose direction: straight or turn - 75% of times the car goes straight\n",
        "                    route_straight = np.random.randint(1, 5)  # choose a random source & destination\n",
        "                    if route_straight == 1:\n",
        "                        print('    <vehicle id=\"W_E_%i\" type=\"standard_car\" route=\"W_E\" depart=\"%s\" departLane=\"random\" departSpeed=\"10\" />' % (car_counter, step), file=routes)\n",
        "                    elif route_straight == 2:\n",
        "                        print('    <vehicle id=\"E_W_%i\" type=\"standard_car\" route=\"E_W\" depart=\"%s\" departLane=\"random\" departSpeed=\"10\" />' % (car_counter, step), file=routes)\n",
        "                    elif route_straight == 3:\n",
        "                        print('    <vehicle id=\"N_S_%i\" type=\"standard_car\" route=\"N_S\" depart=\"%s\" departLane=\"random\" departSpeed=\"10\" />' % (car_counter, step), file=routes)\n",
        "                    else:\n",
        "                        print('    <vehicle id=\"S_N_%i\" type=\"standard_car\" route=\"S_N\" depart=\"%s\" departLane=\"random\" departSpeed=\"10\" />' % (car_counter, step), file=routes)\n",
        "                else:  # car that turn -25% of the time the car turns\n",
        "                    route_turn = np.random.randint(1, 9)  # choose random source source & destination\n",
        "                    if route_turn == 1:\n",
        "                        print('    <vehicle id=\"W_N_%i\" type=\"standard_car\" route=\"W_N\" depart=\"%s\" departLane=\"random\" departSpeed=\"10\" />' % (car_counter, step), file=routes)\n",
        "                    elif route_turn == 2:\n",
        "                        print('    <vehicle id=\"W_S_%i\" type=\"standard_car\" route=\"W_S\" depart=\"%s\" departLane=\"random\" departSpeed=\"10\" />' % (car_counter, step), file=routes)\n",
        "                    elif route_turn == 3:\n",
        "                        print('    <vehicle id=\"N_W_%i\" type=\"standard_car\" route=\"N_W\" depart=\"%s\" departLane=\"random\" departSpeed=\"10\" />' % (car_counter, step), file=routes)\n",
        "                    elif route_turn == 4:\n",
        "                        print('    <vehicle id=\"N_E_%i\" type=\"standard_car\" route=\"N_E\" depart=\"%s\" departLane=\"random\" departSpeed=\"10\" />' % (car_counter, step), file=routes)\n",
        "                    elif route_turn == 5:\n",
        "                        print('    <vehicle id=\"E_N_%i\" type=\"standard_car\" route=\"E_N\" depart=\"%s\" departLane=\"random\" departSpeed=\"10\" />' % (car_counter, step), file=routes)\n",
        "                    elif route_turn == 6:\n",
        "                        print('    <vehicle id=\"E_S_%i\" type=\"standard_car\" route=\"E_S\" depart=\"%s\" departLane=\"random\" departSpeed=\"10\" />' % (car_counter, step), file=routes)\n",
        "                    elif route_turn == 7:\n",
        "                        print('    <vehicle id=\"S_W_%i\" type=\"standard_car\" route=\"S_W\" depart=\"%s\" departLane=\"random\" departSpeed=\"10\" />' % (car_counter, step), file=routes)\n",
        "                    elif route_turn == 8:\n",
        "                        print('    <vehicle id=\"S_E_%i\" type=\"standard_car\" route=\"S_E\" depart=\"%s\" departLane=\"random\" departSpeed=\"10\" />' % (car_counter, step), file=routes)\n",
        "\n",
        "            print(\"</routes>\", file=routes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY1wdF3CR042"
      },
      "source": [
        "class memory:\n",
        "    def __init__(self, size_max, size_min):\n",
        "        self._samples = []\n",
        "        self._size_max = size_max\n",
        "        self._size_min = size_min\n",
        "\n",
        "\n",
        "    def add_sample(self, sample):\n",
        "        \"\"\"\n",
        "        Add a sample into the memory\n",
        "        \"\"\"\n",
        "        self._samples.append(sample)\n",
        "        if self._size_now() > self._size_max:\n",
        "            self._samples.pop(0)  # if the length is greater than the size of memory, remove the oldest element\n",
        "\n",
        "\n",
        "    def get_samples(self, n):\n",
        "        \"\"\"\n",
        "        Get n samples randomly from the memory\n",
        "        \"\"\"\n",
        "        # if self._size_now() < self._size_min:\n",
        "        #     return []\n",
        "\n",
        "        if n > self._size_now():\n",
        "            return random.sample(self._samples, self._size_now())  # get all the samples\n",
        "        else:\n",
        "            return random.sample(self._samples, n)  # get \"batch size\" number of samples\n",
        "\n",
        "\n",
        "    def _size_now(self):\n",
        "        \"\"\"\n",
        "        Check how full the memory is\n",
        "        \"\"\"\n",
        "        return len(self._samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km41hv-tSCGg"
      },
      "source": [
        "class TrainModel:\n",
        "    def __init__(self, num_layers, width, batch_size, learning_rate, input_dim, output_dim):\n",
        "        self._input_dim = input_dim\n",
        "        self._output_dim = output_dim\n",
        "        self._batch_size = batch_size\n",
        "        self._learning_rate = learning_rate\n",
        "        self._model = self._build_model(num_layers, width)\n",
        "\n",
        "\n",
        "    def _build_model(self, num_layers, width):\n",
        "        \"\"\"\n",
        "        Build and compile a fully connected deep neural network\n",
        "        \"\"\"\n",
        "        inputs = keras.Input(shape=(self._input_dim,))\n",
        "        x = layers.Dense(width, activation='relu')(inputs)\n",
        "        for _ in range(num_layers):\n",
        "            x = layers.Dense(width, activation='relu')(x)\n",
        "        outputs = layers.Dense(self._output_dim, activation='linear')(x)\n",
        "\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs, name='my_model')\n",
        "        model.compile(loss=losses.mean_squared_error, optimizer=Adam(lr=self._learning_rate))\n",
        "        return model\n",
        "    \n",
        "\n",
        "    def predict_one(self, state):\n",
        "        \"\"\"\n",
        "        Predict the action values from a single state\n",
        "        \"\"\"\n",
        "        state = np.reshape(state, [1, self._input_dim])\n",
        "        return self._model.predict(state)\n",
        "\n",
        "\n",
        "    def predict_batch(self, states):\n",
        "        \"\"\"\n",
        "        Predict the action values from a batch of states\n",
        "        \"\"\"\n",
        "        return self._model.predict(states)\n",
        "\n",
        "\n",
        "    def train_batch(self, states, q_sa):\n",
        "        \"\"\"\n",
        "        Train the nn using the updated q-values\n",
        "        \"\"\"\n",
        "        self._model.fit(states, q_sa, epochs=1, verbose=0)\n",
        "\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"\n",
        "        Save the current model in the folder as h5 file and a model architecture summary as png\n",
        "        \"\"\"\n",
        "        self._model.save(os.path.join(path, 'trained_model.h5'))\n",
        "        plot_model(self._model, to_file=os.path.join(path, 'model_structure.png'), show_shapes=True, show_layer_names=True)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def input_dim(self):\n",
        "        return self._input_dim\n",
        "\n",
        "\n",
        "    @property\n",
        "    def output_dim(self):\n",
        "        return self._output_dim\n",
        "\n",
        "\n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        return self._batch_size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrIgj1OtSV7p"
      },
      "source": [
        "# phase codes based on environment.net.xml\n",
        "PHASE_NS_GREEN = 0  # action 0 code 00\n",
        "PHASE_NS_YELLOW = 1\n",
        "PHASE_NSL_GREEN = 2  # action 1 code 01\n",
        "PHASE_NSL_YELLOW = 3\n",
        "PHASE_EW_GREEN = 4  # action 2 code 10\n",
        "PHASE_EW_YELLOW = 5\n",
        "PHASE_EWL_GREEN = 6  # action 3 code 11\n",
        "PHASE_EWL_YELLOW = 7\n",
        "\n",
        "\n",
        "class simulation:\n",
        "    def __init__(self, Model, tmodel, Memory, TrafficGen, sumo_cmd, gamma, max_steps, green_duration, yellow_duration, num_states, num_actions, training_epochs):\n",
        "        self._Model = Model\n",
        "        self._tmodel = tmodel\n",
        "        self._Memory = Memory\n",
        "        self._TrafficGen = TrafficGen\n",
        "        self._gamma = gamma\n",
        "        self._step = 0\n",
        "        self._sumo_cmd = sumo_cmd\n",
        "        self._max_steps = max_steps\n",
        "        self._green_duration = green_duration\n",
        "        self._yellow_duration = yellow_duration\n",
        "        self._num_states = num_states\n",
        "        self._num_actions = num_actions\n",
        "        self._reward_store = []\n",
        "        self._cumulative_wait_store = []\n",
        "        self._avg_queue_length_store = []\n",
        "        self._training_epochs = training_epochs\n",
        "\n",
        "\n",
        "    def run(self, epsilon):\n",
        "        \"\"\"\n",
        "        Runs an episode of simulation, then starts a training session\n",
        "        \"\"\"\n",
        "        start_time = timeit.default_timer()\n",
        "\n",
        "        # first, generate the route file for this simulation and set up sumo\n",
        "        self._TrafficGen.generate_routefile(seed=episode)\n",
        "        traci.start(self._sumo_cmd)\n",
        "        print(\"Simulating...\")\n",
        "\n",
        "        # inits\n",
        "        self._step = 0\n",
        "        self._waiting_times = {}\n",
        "        self._sum_neg_reward = 0\n",
        "        self._sum_queue_length = 0\n",
        "        self._sum_waiting_time = 0\n",
        "        old_total_wait = 0\n",
        "        old_state = -1\n",
        "        old_action = -1\n",
        "\n",
        "        while self._step < self._max_steps:\n",
        "\n",
        "            # get current state of the intersection\n",
        "            current_state = self._get_state()\n",
        "\n",
        "            # calculate reward of previous action: (change in cumulative waiting time between actions)\n",
        "            # waiting time = seconds waited by a car since the spawn in the environment, cumulated for every car in incoming lanes\n",
        "            current_total_wait = self._collect_waiting_times()\n",
        "            reward = old_total_wait - current_total_wait\n",
        "\n",
        "            # saving the data into the memory\n",
        "            if self._step != 0:\n",
        "                self._Memory.add_sample((old_state, old_action, reward, current_state))\n",
        "                # print(\"add to memory\")\n",
        "\n",
        "            # choose the light phase to activate, based on the current state of the intersection\n",
        "            action = self._choose_action(current_state, epsilon)\n",
        "\n",
        "            # if the chosen phase is different from the last phase, activate the yellow phase\n",
        "            if self._step != 0 and old_action != action:\n",
        "                self._set_yellow_phase(old_action)\n",
        "                self._simulate(self._yellow_duration)\n",
        "\n",
        "            # execute the phase selected before\n",
        "            self._set_green_phase(action)\n",
        "            self._simulate(self._green_duration)\n",
        "\n",
        "            # saving variables for later & accumulate reward\n",
        "            old_state = current_state\n",
        "            old_action = action\n",
        "            old_total_wait = current_total_wait\n",
        "\n",
        "            # saving only the meaningful reward to better see if the agent is behaving correctly\n",
        "            if reward < 0:\n",
        "                self._sum_neg_reward += reward\n",
        "\n",
        "            \n",
        "            # if self._step % 200 == 0:\n",
        "            #     target_nn.set_weights(main_nn.get_weights())\n",
        "            if self._step % 5000 == 0:\n",
        "                self._tmodel.target_train()\n",
        "\n",
        "        self._save_episode_stats()\n",
        "        print(\"Total reward:\", self._sum_neg_reward, \"- Epsilon:\", round(epsilon, 2))\n",
        "        traci.close()\n",
        "        simulation_time = round(timeit.default_timer() - start_time, 1)\n",
        "\n",
        "        print(\"Training...\")\n",
        "        # start_time = timeit.default_timer()\n",
        "        # for _ in range(self._training_epochs):\n",
        "        #     self._replay()\n",
        "        batch = self._Memory.get_samples(100)\n",
        "        # print(\"******\")\n",
        "        # print(batch)\n",
        "        # print(\"******\")\n",
        "        training_time = round(timeit.default_timer() - start_time, 1)\n",
        "\n",
        "        return simulation_time, training_time,\n",
        "\n",
        "\n",
        "    def _simulate(self, steps_todo):\n",
        "        \"\"\"\n",
        "        Execute steps in sumo while gathering statistics\n",
        "        \"\"\"\n",
        "        if (self._step + steps_todo) >= self._max_steps:  # do not do more steps than the maximum allowed number of steps\n",
        "            steps_todo = self._max_steps - self._step\n",
        "\n",
        "        while steps_todo > 0:\n",
        "            traci.simulationStep()  # simulate 1 step in sumo\n",
        "            self._step += 1 # update the step counter\n",
        "            steps_todo -= 1\n",
        "            queue_length = self._get_queue_length()\n",
        "            self._sum_queue_length += queue_length\n",
        "            self._sum_waiting_time += queue_length # 1 step while wating in queue means 1 second waited, for each car, therefore queue_lenght == waited_seconds\n",
        "\n",
        "\n",
        "    def _collect_waiting_times(self):\n",
        "        \"\"\"\n",
        "        Retrieve the waiting time of every car in the incoming roads\n",
        "        \"\"\"\n",
        "        incoming_roads = [\"E2TL\", \"N2TL\", \"W2TL\", \"S2TL\"]\n",
        "        car_list = traci.vehicle.getIDList()\n",
        "        for car_id in car_list:\n",
        "            wait_time = traci.vehicle.getAccumulatedWaitingTime(car_id)\n",
        "            road_id = traci.vehicle.getRoadID(car_id)  # get the road id where the car is located\n",
        "            if road_id in incoming_roads:  # consider only the waiting times of cars in incoming roads\n",
        "                self._waiting_times[car_id] = wait_time\n",
        "            else:\n",
        "                if car_id in self._waiting_times: # a car that was tracked has cleared the intersection\n",
        "                    del self._waiting_times[car_id] \n",
        "        total_waiting_time = sum(self._waiting_times.values())\n",
        "        return total_waiting_time\n",
        "\n",
        "\n",
        "    def _choose_action(self, state, epsilon):\n",
        "        \"\"\"\n",
        "        Decide wheter to perform an explorative or exploitative action, according to an epsilon-greedy policy\n",
        "        \"\"\"\n",
        "        if random.random() < epsilon:\n",
        "            return random.randint(0, self._num_actions - 1) # random action\n",
        "        else:\n",
        "            # return np.argmax(self._Model.predict_one(state)) # the best action given the current state\n",
        "            return np.argmax(self._tmodel.predict_one(state))\n",
        "\n",
        "\n",
        "    def _set_yellow_phase(self, old_action):\n",
        "        \"\"\"\n",
        "        Activate the correct yellow light combination in sumo\n",
        "        \"\"\"\n",
        "        yellow_phase_code = old_action * 2 + 1 # obtain the yellow phase code, based on the old action (ref on environment.net.xml)\n",
        "        traci.trafficlight.setPhase(\"TL\", yellow_phase_code)\n",
        "\n",
        "\n",
        "    def _set_green_phase(self, action_number):\n",
        "        \"\"\"\n",
        "        Activate the correct green light combination in sumo\n",
        "        \"\"\"\n",
        "        if action_number == 0:\n",
        "            traci.trafficlight.setPhase(\"TL\", PHASE_NS_GREEN)\n",
        "        elif action_number == 1:\n",
        "            traci.trafficlight.setPhase(\"TL\", PHASE_NSL_GREEN)\n",
        "        elif action_number == 2:\n",
        "            traci.trafficlight.setPhase(\"TL\", PHASE_EW_GREEN)\n",
        "        elif action_number == 3:\n",
        "            traci.trafficlight.setPhase(\"TL\", PHASE_EWL_GREEN)\n",
        "\n",
        "\n",
        "    def _get_queue_length(self):\n",
        "        \"\"\"\n",
        "        Retrieve the number of cars with speed = 0 in every incoming lane\n",
        "        \"\"\"\n",
        "        halt_N = traci.edge.getLastStepHaltingNumber(\"N2TL\")\n",
        "        halt_S = traci.edge.getLastStepHaltingNumber(\"S2TL\")\n",
        "        halt_E = traci.edge.getLastStepHaltingNumber(\"E2TL\")\n",
        "        halt_W = traci.edge.getLastStepHaltingNumber(\"W2TL\")\n",
        "        queue_length = halt_N + halt_S + halt_E + halt_W\n",
        "        return queue_length\n",
        "\n",
        "\n",
        "    def _get_state(self):\n",
        "        \"\"\"\n",
        "        Retrieve the state of the intersection from sumo, in the form of cell occupancy\n",
        "        \"\"\"\n",
        "        state = np.zeros(self._num_states)\n",
        "        car_list = traci.vehicle.getIDList()\n",
        "\n",
        "        for car_id in car_list:\n",
        "            lane_pos = traci.vehicle.getLanePosition(car_id)\n",
        "            lane_id = traci.vehicle.getLaneID(car_id)\n",
        "            lane_pos = 750 - lane_pos  # inversion of lane pos, so if the car is close to the traffic light -> lane_pos = 0 --- 750 = max len of a road\n",
        "\n",
        "            # distance in meters from the traffic light -> mapping into cells\n",
        "            if lane_pos < 7:\n",
        "                lane_cell = 0\n",
        "            elif lane_pos < 14:\n",
        "                lane_cell = 1\n",
        "            elif lane_pos < 21:\n",
        "                lane_cell = 2\n",
        "            elif lane_pos < 28:\n",
        "                lane_cell = 3\n",
        "            elif lane_pos < 40:\n",
        "                lane_cell = 4\n",
        "            elif lane_pos < 60:\n",
        "                lane_cell = 5\n",
        "            elif lane_pos < 100:\n",
        "                lane_cell = 6\n",
        "            elif lane_pos < 160:\n",
        "                lane_cell = 7\n",
        "            elif lane_pos < 400:\n",
        "                lane_cell = 8\n",
        "            elif lane_pos <= 750:\n",
        "                lane_cell = 9\n",
        "\n",
        "            # finding the lane where the car is located \n",
        "            # x2TL_3 are the \"turn left only\" lanes\n",
        "            if lane_id == \"W2TL_0\" or lane_id == \"W2TL_1\" or lane_id == \"W2TL_2\":\n",
        "                lane_group = 0\n",
        "            elif lane_id == \"W2TL_3\":\n",
        "                lane_group = 1\n",
        "            elif lane_id == \"N2TL_0\" or lane_id == \"N2TL_1\" or lane_id == \"N2TL_2\":\n",
        "                lane_group = 2\n",
        "            elif lane_id == \"N2TL_3\":\n",
        "                lane_group = 3\n",
        "            elif lane_id == \"E2TL_0\" or lane_id == \"E2TL_1\" or lane_id == \"E2TL_2\":\n",
        "                lane_group = 4\n",
        "            elif lane_id == \"E2TL_3\":\n",
        "                lane_group = 5\n",
        "            elif lane_id == \"S2TL_0\" or lane_id == \"S2TL_1\" or lane_id == \"S2TL_2\":\n",
        "                lane_group = 6\n",
        "            elif lane_id == \"S2TL_3\":\n",
        "                lane_group = 7\n",
        "            else:\n",
        "                lane_group = -1\n",
        "\n",
        "            if lane_group >= 1 and lane_group <= 7:\n",
        "                car_position = int(str(lane_group) + str(lane_cell))  # composition of the two postion ID to create a number in interval 0-79\n",
        "                valid_car = True\n",
        "            elif lane_group == 0:\n",
        "                car_position = lane_cell\n",
        "                valid_car = True\n",
        "            else:\n",
        "                valid_car = False  # flag for not detecting cars crossing the intersection or driving away from it\n",
        "\n",
        "            if valid_car:\n",
        "                state[car_position] = 1  # write the position of the car car_id in the state array in the form of \"cell occupied\"\n",
        "\n",
        "        return state\n",
        "\n",
        "\n",
        "    def _replay(self):\n",
        "        \"\"\"\n",
        "        Retrieve a group of samples from the memory and for each of them update the learning equation, then train\n",
        "        \"\"\"\n",
        "        batch = self._Memory.get_samples(self._Model.batch_size)\n",
        "\n",
        "        if len(batch) > 0:  # if the memory is full enough\n",
        "            states = np.array([val[0] for val in batch])  # extract states from the batch\n",
        "            next_states = np.array([val[3] for val in batch])  # extract next states from the batch\n",
        "\n",
        "            # prediction\n",
        "            q_s_a = self._Model.predict_batch(states)  # predict Q(state), for every sample\n",
        "            q_s_a_d = self._Model.predict_batch(next_states)  # predict Q(next_state), for every sample\n",
        "\n",
        "            # setup training arrays\n",
        "            x = np.zeros((len(batch), self._num_states))\n",
        "            y = np.zeros((len(batch), self._num_actions))\n",
        "\n",
        "            for i, b in enumerate(batch):\n",
        "                state, action, reward, _ = b[0], b[1], b[2], b[3]  # extract data from one sample\n",
        "                current_q = q_s_a[i]  # get the Q(state) predicted before\n",
        "                current_q[action] = reward + self._gamma * np.amax(q_s_a_d[i])  # update Q(state, action)\n",
        "                x[i] = state\n",
        "                y[i] = current_q  # Q(state) that includes the updated action value\n",
        "\n",
        "            self._Model.train_batch(x, y)  # train the NN\n",
        "\n",
        "\n",
        "    def _save_episode_stats(self):\n",
        "        \"\"\"\n",
        "        Save the stats of the episode to plot the graphs at the end of the session\n",
        "        \"\"\"\n",
        "        self._reward_store.append(self._sum_neg_reward)  # how much negative reward in this episode\n",
        "        self._cumulative_wait_store.append(self._sum_waiting_time)  # total number of seconds waited by cars in this episode\n",
        "        self._avg_queue_length_store.append(self._sum_queue_length / self._max_steps)  # average number of queued cars per step, in this episode\n",
        "\n",
        "\n",
        "    @property\n",
        "    def reward_store(self):\n",
        "        return self._reward_store\n",
        "\n",
        "\n",
        "    @property\n",
        "    def cumulative_wait_store(self):\n",
        "        return self._cumulative_wait_store\n",
        "\n",
        "\n",
        "    @property\n",
        "    def avg_queue_length_store(self):\n",
        "        return self._avg_queue_length_store"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0rGjVFbTsEo"
      },
      "source": [
        "# Utils\n",
        "import configparser\n",
        "from sumolib import checkBinary\n",
        "\n",
        "def import_train_configuration(config_file):\n",
        "    \"\"\"\n",
        "    Read the config file regarding the training and import its content\n",
        "    \"\"\"\n",
        "    content = configparser.ConfigParser()\n",
        "    content.read(config_file)\n",
        "    config = {}\n",
        "    config['gui'] = content['simulation'].getboolean('gui')\n",
        "    config['total_episodes'] = content['simulation'].getint('total_episodes')\n",
        "    config['max_steps'] = content['simulation'].getint('max_steps')\n",
        "    config['n_cars_generated'] = content['simulation'].getint('n_cars_generated')\n",
        "    config['green_duration'] = content['simulation'].getint('green_duration')\n",
        "    config['yellow_duration'] = content['simulation'].getint('yellow_duration')\n",
        "    config['num_layers'] = content['model'].getint('num_layers')\n",
        "    config['width_layers'] = content['model'].getint('width_layers')\n",
        "    config['batch_size'] = content['model'].getint('batch_size')\n",
        "    config['learning_rate'] = content['model'].getfloat('learning_rate')\n",
        "    config['training_epochs'] = content['model'].getint('training_epochs')\n",
        "    config['memory_size_min'] = content['memory'].getint('memory_size_min')\n",
        "    config['memory_size_max'] = content['memory'].getint('memory_size_max')\n",
        "    config['num_states'] = content['agent'].getint('num_states')\n",
        "    config['num_actions'] = content['agent'].getint('num_actions')\n",
        "    config['gamma'] = content['agent'].getfloat('gamma')\n",
        "    config['models_path_name'] = content['dir']['models_path_name']\n",
        "    config['sumocfg_file_name'] = content['dir']['sumocfg_file_name']\n",
        "    return config\n",
        "\n",
        "\n",
        "def import_test_configuration(config_file):\n",
        "    \"\"\"\n",
        "    Read the config file regarding the testing and import its content\n",
        "    \"\"\"\n",
        "    content = configparser.ConfigParser()\n",
        "    content.read(config_file)\n",
        "    config = {}\n",
        "    config['gui'] = content['simulation'].getboolean('gui')\n",
        "    config['max_steps'] = content['simulation'].getint('max_steps')\n",
        "    config['n_cars_generated'] = content['simulation'].getint('n_cars_generated')\n",
        "    config['episode_seed'] = content['simulation'].getint('episode_seed')\n",
        "    config['green_duration'] = content['simulation'].getint('green_duration')\n",
        "    config['yellow_duration'] = content['simulation'].getint('yellow_duration')\n",
        "    config['num_states'] = content['agent'].getint('num_states')\n",
        "    config['num_actions'] = content['agent'].getint('num_actions')\n",
        "    config['sumocfg_file_name'] = content['dir']['sumocfg_file_name']\n",
        "    config['models_path_name'] = content['dir']['models_path_name']\n",
        "    config['model_to_test'] = content['dir'].getint('model_to_test') \n",
        "    return config\n",
        "\n",
        "\n",
        "def set_sumo(gui, sumocfg_file_name, max_steps):\n",
        "    \"\"\"\n",
        "    Configure various parameters of SUMO\n",
        "    \"\"\"\n",
        "    # sumo things - we need to import python modules from the $SUMO_HOME/tools directory\n",
        "    if 'SUMO_HOME' in os.environ:\n",
        "        tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
        "        sys.path.append(tools)\n",
        "    else:\n",
        "        sys.exit(\"please declare environment variable 'SUMO_HOME'\")\n",
        "\n",
        "    # setting the cmd mode or the visual mode    \n",
        "    if gui == False:\n",
        "        sumoBinary = checkBinary('sumo')\n",
        "    else:\n",
        "        sumoBinary = checkBinary('sumo-gui')\n",
        " \n",
        "    # setting the cmd command to run sumo at simulation time\n",
        "    sumo_cmd = [sumoBinary, \"-c\", os.path.join('intersection', sumocfg_file_name), \"--no-step-log\", \"true\", \"--waiting-time-memory\", str(max_steps)]\n",
        "\n",
        "    return sumo_cmd\n",
        "\n",
        "\n",
        "def set_train_path(models_path_name):\n",
        "    \"\"\"\n",
        "    Create a new model path with an incremental integer, also considering previously created model paths\n",
        "    \"\"\"\n",
        "    models_path = os.path.join(os.getcwd(), models_path_name, '')\n",
        "    os.makedirs(os.path.dirname(models_path), exist_ok=True)\n",
        "\n",
        "    dir_content = os.listdir(models_path)\n",
        "    if dir_content:\n",
        "        previous_versions = [int(name.split(\"_\")[1]) for name in dir_content]\n",
        "        new_version = str(max(previous_versions) + 1)\n",
        "    else:\n",
        "        new_version = '1'\n",
        "\n",
        "    data_path = os.path.join(models_path, 'model_'+new_version, '')\n",
        "    os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
        "    return data_path \n",
        "\n",
        "\n",
        "def set_test_path(models_path_name, model_n):\n",
        "    \"\"\"\n",
        "    Returns a model path that identifies the model number provided as argument and a newly created 'test' path\n",
        "    \"\"\"\n",
        "    model_folder_path = os.path.join(os.getcwd(), models_path_name, 'model_'+str(model_n), '')\n",
        "\n",
        "    if os.path.isdir(model_folder_path):    \n",
        "        plot_path = os.path.join(model_folder_path, 'test', '')\n",
        "        os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
        "        return model_folder_path, plot_path\n",
        "    else: \n",
        "        sys.exit('The model number specified does not exist in the models folder')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27U68emKUB_2"
      },
      "source": [
        "class visualization:\n",
        "    def __init__(self, path, dpi):\n",
        "            self._path = path\n",
        "            self._dpi = dpi\n",
        "\n",
        "\n",
        "    def save_data_and_plot(self, data, filename, xlabel, ylabel):\n",
        "        \"\"\"\n",
        "        Produce a plot of performance of the agent over the session and save the relative data to txt\n",
        "        \"\"\"\n",
        "        min_val = min(data)\n",
        "        max_val = max(data)\n",
        "\n",
        "        plt.rcParams.update({'font.size': 24})  # set bigger font size\n",
        "\n",
        "        plt.plot(data)\n",
        "        plt.ylabel(ylabel)\n",
        "        plt.xlabel(xlabel)\n",
        "        plt.margins(0)\n",
        "        plt.ylim(min_val - 0.05 * abs(min_val), max_val + 0.05 * abs(max_val))\n",
        "        fig = plt.gcf()\n",
        "        fig.set_size_inches(20, 11.25)\n",
        "        fig.savefig(os.path.join(self._path, 'plot_'+filename+'.png'), dpi=self._dpi)\n",
        "        plt.close(\"all\")\n",
        "\n",
        "        with open(os.path.join(self._path, 'plot_'+filename + '_data.txt'), \"w\") as file:\n",
        "            for value in data:\n",
        "                    file.write(\"%s\\n\" % value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSf2X-XV307N"
      },
      "source": [
        "class DQN:\n",
        "    def __init__(self, learning_rate, input_dim, output_dim, gamma):\n",
        "        self._learning_rate = learning_rate\n",
        "        self._input_dim = input_dim\n",
        "        self._output_dim = output_dim\n",
        "        self._gamma = gamma\n",
        "        self.tau = .125\n",
        "        \n",
        "\n",
        "        self.model        = self.create_model()\n",
        "        self.target_model = self.create_model()\n",
        "\n",
        "\n",
        "    def create_model(self):\n",
        "        input = keras.Input(shape=(self._input_dim,))\n",
        "        x = keras.layers.Flatten()(input)\n",
        "        x = keras.layers.Dense(400, activation='relu')(x)\n",
        "        x = keras.layers.Dense(400, activation='relu')(x)\n",
        "        x = keras.layers.Dense(400, activation='relu')(x)\n",
        "        x = keras.layers.Dense(400, activation='relu')(x)\n",
        "        output = keras.layers.Dense(self._output_dim, activation='linear')(x)\n",
        "        model = keras.Model(inputs=input, outputs=output)\n",
        "        model.compile(loss=losses.mean_squared_error, optimizer=Adam(lr=self._learning_rate))\n",
        "        return model\n",
        "\n",
        "\n",
        "    def predict_one(self, state):\n",
        "        state = np.reshape(state, [1, self._input_dim])\n",
        "        return self.model.predict(state)\n",
        "\n",
        "    def replay(self, batch):\n",
        "        states = np.array([val[0] for val in batch])  # extract states from the batch\n",
        "        next_states = np.array([val[3] for val in batch])  # extract next states from the batch\n",
        "        target = self.target_model.predict(states)\n",
        "        Q_future = self.target_model.predict(next_states)\n",
        "        x = np.zeros((len(batch), 80))\n",
        "        y = np.zeros((len(batch), 4))\n",
        "        for i, b in enumerate(batch):\n",
        "            state, action, reward, _ = b[0], b[1], b[2], b[3]\n",
        "            current_q = target[i]\n",
        "            current_q[action] = reward + self._gamma * np.amax(Q_future[i])\n",
        "            x[i] = state\n",
        "            y[i] = current_q\n",
        "        self.model.fit(x, y, epochs=800, verbose=0)\n",
        "  \n",
        "    def target_train(self):\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        for i in range(len(target_weights)):\n",
        "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
        "        self.target_model.set_weights(target_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebzkYv4_klSV"
      },
      "source": [
        "**Starting Training Process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2ACqSSajWoT",
        "outputId": "94b5112a-b6af-4d7d-aaa9-20e72c54eafa"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    config = import_train_configuration(config_file='/content/train.ini')\n",
        "    # sumo_cmd = set_sumo(config['gui'], config['sumocfg_file_name'], config['max_steps'])\n",
        "    sumo_file_path = \"/content/sumo_config.sumocfg\"\n",
        "    sumo_cmd = set_sumo(False, sumo_file_path, 5400)\n",
        "    path = set_train_path(config['models_path_name'])\n",
        "    num_states = 80\n",
        "    num_actions = 4\n",
        "\n",
        "    model_nn = DQN(\n",
        "            config['learning_rate'], \n",
        "            config['num_states'], \n",
        "            config['num_actions'],\n",
        "            config['gamma']\n",
        "    )\n",
        "\n",
        "    target_nn = DQN(\n",
        "        config['learning_rate'], \n",
        "        config['num_states'], \n",
        "        config['num_actions'],\n",
        "        config['gamma'] \n",
        "    )\n",
        "\n",
        "    Memory = memory(600, 50000)\n",
        "\n",
        "    TrafficGen = TrafficGenerator(\n",
        "        config['max_steps'], \n",
        "        config['n_cars_generated']\n",
        "    )\n",
        "\n",
        "    Visualization = visualization(\n",
        "        path, \n",
        "        dpi=96\n",
        "    )\n",
        "        \n",
        "    Simulation = simulation(\n",
        "        model_nn,\n",
        "        target_nn,\n",
        "        Memory,\n",
        "        TrafficGen,\n",
        "        sumo_cmd,\n",
        "        config['gamma'],\n",
        "        config['max_steps'],\n",
        "        config['green_duration'],\n",
        "        config['yellow_duration'],\n",
        "        config['num_states'],\n",
        "        config['num_actions'],\n",
        "        config['training_epochs']\n",
        "    )\n",
        "    \n",
        "    episode = 0\n",
        "    timestamp_start = datetime.datetime.now()\n",
        "    step = 0\n",
        "    # max_steps = 10\n",
        "    while episode < config['total_episodes']:\n",
        "        print('\\n----- Episode', str(episode+1), 'of', str(config['total_episodes']))\n",
        "        epsilon = 1.0 - (episode / config['total_episodes'])  # set the epsilon for this episode according to epsilon-greedy policy\n",
        " #************************************#\n",
        "        # while step < max_steps:\n",
        "        step = step + 1\n",
        "        print(step)\n",
        "        simulation_time, training_time = Simulation.run(epsilon)  # run the simulation\n",
        "        # if step % 2000 == 0:\n",
        "        #     target_nn.target_train()\n",
        "        \n",
        "        batch = Memory.get_samples(100)\n",
        "        #print(batch)\n",
        "        if len(batch) > 0:\n",
        "            \n",
        "            model_nn.replay(batch)\n",
        "            # step += step\n",
        "        print('Simulation time:', simulation_time, 's - Training time:', training_time, 's - Total:', round(simulation_time+training_time, 1), 's')\n",
        "        episode += 1\n",
        "\n",
        "    print(\"\\n----- Start time:\", timestamp_start)\n",
        "    print(\"----- End time:\", datetime.datetime.now())\n",
        "    print(\"----- Session info saved at:\", path)\n",
        "\n",
        "    Model.save_model(path)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----- Episode 1 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -26438.0 - Epsilon: 1.0\n",
            "Training...\n",
            "Simulation time: 7.5 s - Training time: 0.0 s - Total: 7.5 s\n",
            "\n",
            "----- Episode 2 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -33185.0 - Epsilon: 0.99\n",
            "Training...\n",
            "Simulation time: 9.2 s - Training time: 81.1 s - Total: 90.3 s\n",
            "\n",
            "----- Episode 3 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -24200.0 - Epsilon: 0.98\n",
            "Training...\n",
            "Simulation time: 7.1 s - Training time: 78.3 s - Total: 85.4 s\n",
            "\n",
            "----- Episode 4 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -46069.0 - Epsilon: 0.97\n",
            "Training...\n",
            "Simulation time: 7.6 s - Training time: 79.0 s - Total: 86.6 s\n",
            "\n",
            "----- Episode 5 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -42239.0 - Epsilon: 0.96\n",
            "Training...\n",
            "Simulation time: 7.8 s - Training time: 78.1 s - Total: 85.9 s\n",
            "\n",
            "----- Episode 6 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -29656.0 - Epsilon: 0.95\n",
            "Training...\n",
            "Simulation time: 7.7 s - Training time: 77.3 s - Total: 85.0 s\n",
            "\n",
            "----- Episode 7 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -19485.0 - Epsilon: 0.94\n",
            "Training...\n",
            "Simulation time: 7.5 s - Training time: 77.6 s - Total: 85.1 s\n",
            "\n",
            "----- Episode 8 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -25578.0 - Epsilon: 0.93\n",
            "Training...\n",
            "Simulation time: 7.8 s - Training time: 78.2 s - Total: 86.0 s\n",
            "\n",
            "----- Episode 9 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -17638.0 - Epsilon: 0.92\n",
            "Training...\n",
            "Simulation time: 7.8 s - Training time: 77.6 s - Total: 85.4 s\n",
            "\n",
            "----- Episode 10 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -21329.0 - Epsilon: 0.91\n",
            "Training...\n",
            "Simulation time: 8.0 s - Training time: 77.7 s - Total: 85.7 s\n",
            "\n",
            "----- Episode 11 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -30736.0 - Epsilon: 0.9\n",
            "Training...\n",
            "Simulation time: 8.0 s - Training time: 78.2 s - Total: 86.2 s\n",
            "\n",
            "----- Episode 12 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -18163.0 - Epsilon: 0.89\n",
            "Training...\n",
            "Simulation time: 7.8 s - Training time: 77.9 s - Total: 85.7 s\n",
            "\n",
            "----- Episode 13 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -18360.0 - Epsilon: 0.88\n",
            "Training...\n",
            "Simulation time: 8.4 s - Training time: 77.5 s - Total: 85.9 s\n",
            "\n",
            "----- Episode 14 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -24438.0 - Epsilon: 0.87\n",
            "Training...\n",
            "Simulation time: 8.4 s - Training time: 76.5 s - Total: 84.9 s\n",
            "\n",
            "----- Episode 15 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -16326.0 - Epsilon: 0.86\n",
            "Training...\n",
            "Simulation time: 8.0 s - Training time: 77.5 s - Total: 85.5 s\n",
            "\n",
            "----- Episode 16 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -17987.0 - Epsilon: 0.85\n",
            "Training...\n",
            "Simulation time: 8.3 s - Training time: 76.8 s - Total: 85.1 s\n",
            "\n",
            "----- Episode 17 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -16441.0 - Epsilon: 0.84\n",
            "Training...\n",
            "Simulation time: 8.5 s - Training time: 76.2 s - Total: 84.7 s\n",
            "\n",
            "----- Episode 18 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -18623.0 - Epsilon: 0.83\n",
            "Training...\n",
            "Simulation time: 8.5 s - Training time: 76.2 s - Total: 84.7 s\n",
            "\n",
            "----- Episode 19 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -20102.0 - Epsilon: 0.82\n",
            "Training...\n",
            "Simulation time: 9.1 s - Training time: 76.8 s - Total: 85.9 s\n",
            "\n",
            "----- Episode 20 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -15517.0 - Epsilon: 0.81\n",
            "Training...\n",
            "Simulation time: 8.8 s - Training time: 76.4 s - Total: 85.2 s\n",
            "\n",
            "----- Episode 21 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -12870.0 - Epsilon: 0.8\n",
            "Training...\n",
            "Simulation time: 8.9 s - Training time: 76.4 s - Total: 85.3 s\n",
            "\n",
            "----- Episode 22 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -16787.0 - Epsilon: 0.79\n",
            "Training...\n",
            "Simulation time: 9.3 s - Training time: 77.1 s - Total: 86.4 s\n",
            "\n",
            "----- Episode 23 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Warning: Choosing new speed factor 1.25 for vehicle 'N_E_255' to match departure speed.\n",
            "Total reward: -19579.0 - Epsilon: 0.78\n",
            "Training...\n",
            "Simulation time: 9.2 s - Training time: 76.8 s - Total: 86.0 s\n",
            "\n",
            "----- Episode 24 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -14153.0 - Epsilon: 0.77\n",
            "Training...\n",
            "Simulation time: 9.6 s - Training time: 76.8 s - Total: 86.4 s\n",
            "\n",
            "----- Episode 25 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -22215.0 - Epsilon: 0.76\n",
            "Training...\n",
            "Simulation time: 9.7 s - Training time: 76.6 s - Total: 86.3 s\n",
            "\n",
            "----- Episode 26 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -14666.0 - Epsilon: 0.75\n",
            "Training...\n",
            "Simulation time: 9.6 s - Training time: 77.4 s - Total: 87.0 s\n",
            "\n",
            "----- Episode 27 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -16561.0 - Epsilon: 0.74\n",
            "Training...\n",
            "Simulation time: 9.1 s - Training time: 76.5 s - Total: 85.6 s\n",
            "\n",
            "----- Episode 28 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -11466.0 - Epsilon: 0.73\n",
            "Training...\n",
            "Simulation time: 9.6 s - Training time: 76.5 s - Total: 86.1 s\n",
            "\n",
            "----- Episode 29 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -12207.0 - Epsilon: 0.72\n",
            "Training...\n",
            "Simulation time: 9.7 s - Training time: 77.2 s - Total: 86.9 s\n",
            "\n",
            "----- Episode 30 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -13704.0 - Epsilon: 0.71\n",
            "Training...\n",
            "Simulation time: 9.4 s - Training time: 76.4 s - Total: 85.8 s\n",
            "\n",
            "----- Episode 31 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -11517.0 - Epsilon: 0.7\n",
            "Training...\n",
            "Simulation time: 9.6 s - Training time: 76.2 s - Total: 85.8 s\n",
            "\n",
            "----- Episode 32 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -12302.0 - Epsilon: 0.69\n",
            "Training...\n",
            "Simulation time: 9.7 s - Training time: 76.0 s - Total: 85.7 s\n",
            "\n",
            "----- Episode 33 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -12683.0 - Epsilon: 0.68\n",
            "Training...\n",
            "Simulation time: 9.6 s - Training time: 76.5 s - Total: 86.1 s\n",
            "\n",
            "----- Episode 34 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -13448.0 - Epsilon: 0.67\n",
            "Training...\n",
            "Simulation time: 10.1 s - Training time: 76.3 s - Total: 86.4 s\n",
            "\n",
            "----- Episode 35 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -14464.0 - Epsilon: 0.66\n",
            "Training...\n",
            "Simulation time: 10.2 s - Training time: 76.2 s - Total: 86.4 s\n",
            "\n",
            "----- Episode 36 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -13865.0 - Epsilon: 0.65\n",
            "Training...\n",
            "Simulation time: 10.2 s - Training time: 75.9 s - Total: 86.1 s\n",
            "\n",
            "----- Episode 37 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -9475.0 - Epsilon: 0.64\n",
            "Training...\n",
            "Simulation time: 10.3 s - Training time: 76.5 s - Total: 86.8 s\n",
            "\n",
            "----- Episode 38 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -12716.0 - Epsilon: 0.63\n",
            "Training...\n",
            "Simulation time: 10.6 s - Training time: 76.0 s - Total: 86.6 s\n",
            "\n",
            "----- Episode 39 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -9583.0 - Epsilon: 0.62\n",
            "Training...\n",
            "Simulation time: 10.1 s - Training time: 75.5 s - Total: 85.6 s\n",
            "\n",
            "----- Episode 40 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -11542.0 - Epsilon: 0.61\n",
            "Training...\n",
            "Simulation time: 10.9 s - Training time: 76.0 s - Total: 86.9 s\n",
            "\n",
            "----- Episode 41 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -13153.0 - Epsilon: 0.6\n",
            "Training...\n",
            "Simulation time: 10.7 s - Training time: 76.3 s - Total: 87.0 s\n",
            "\n",
            "----- Episode 42 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -10318.0 - Epsilon: 0.59\n",
            "Training...\n",
            "Simulation time: 11.3 s - Training time: 77.3 s - Total: 88.6 s\n",
            "\n",
            "----- Episode 43 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -10893.0 - Epsilon: 0.58\n",
            "Training...\n",
            "Simulation time: 11.2 s - Training time: 80.3 s - Total: 91.5 s\n",
            "\n",
            "----- Episode 44 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -11401.0 - Epsilon: 0.57\n",
            "Training...\n",
            "Simulation time: 12.0 s - Training time: 80.1 s - Total: 92.1 s\n",
            "\n",
            "----- Episode 45 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -13764.0 - Epsilon: 0.56\n",
            "Training...\n",
            "Simulation time: 11.9 s - Training time: 78.4 s - Total: 90.3 s\n",
            "\n",
            "----- Episode 46 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -9667.0 - Epsilon: 0.55\n",
            "Training...\n",
            "Simulation time: 11.7 s - Training time: 77.8 s - Total: 89.5 s\n",
            "\n",
            "----- Episode 47 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -11799.0 - Epsilon: 0.54\n",
            "Training...\n",
            "Simulation time: 11.7 s - Training time: 78.6 s - Total: 90.3 s\n",
            "\n",
            "----- Episode 48 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -7702.0 - Epsilon: 0.53\n",
            "Training...\n",
            "Simulation time: 11.6 s - Training time: 76.1 s - Total: 87.7 s\n",
            "\n",
            "----- Episode 49 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -10287.0 - Epsilon: 0.52\n",
            "Training...\n",
            "Simulation time: 11.7 s - Training time: 76.0 s - Total: 87.7 s\n",
            "\n",
            "----- Episode 50 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -8920.0 - Epsilon: 0.51\n",
            "Training...\n",
            "Simulation time: 11.7 s - Training time: 78.0 s - Total: 89.7 s\n",
            "\n",
            "----- Episode 51 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -6897.0 - Epsilon: 0.5\n",
            "Training...\n",
            "Simulation time: 12.3 s - Training time: 78.6 s - Total: 90.9 s\n",
            "\n",
            "----- Episode 52 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -8728.0 - Epsilon: 0.49\n",
            "Training...\n",
            "Simulation time: 12.1 s - Training time: 78.6 s - Total: 90.7 s\n",
            "\n",
            "----- Episode 53 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -8343.0 - Epsilon: 0.48\n",
            "Training...\n",
            "Simulation time: 12.8 s - Training time: 79.2 s - Total: 92.0 s\n",
            "\n",
            "----- Episode 54 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -9334.0 - Epsilon: 0.47\n",
            "Training...\n",
            "Simulation time: 12.6 s - Training time: 79.6 s - Total: 92.2 s\n",
            "\n",
            "----- Episode 55 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -9208.0 - Epsilon: 0.46\n",
            "Training...\n",
            "Simulation time: 13.4 s - Training time: 78.1 s - Total: 91.5 s\n",
            "\n",
            "----- Episode 56 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -7662.0 - Epsilon: 0.45\n",
            "Training...\n",
            "Simulation time: 12.9 s - Training time: 78.7 s - Total: 91.6 s\n",
            "\n",
            "----- Episode 57 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -11257.0 - Epsilon: 0.44\n",
            "Training...\n",
            "Simulation time: 12.6 s - Training time: 79.6 s - Total: 92.2 s\n",
            "\n",
            "----- Episode 58 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -7560.0 - Epsilon: 0.43\n",
            "Training...\n",
            "Simulation time: 12.9 s - Training time: 79.3 s - Total: 92.2 s\n",
            "\n",
            "----- Episode 59 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -7874.0 - Epsilon: 0.42\n",
            "Training...\n",
            "Simulation time: 12.6 s - Training time: 79.3 s - Total: 91.9 s\n",
            "\n",
            "----- Episode 60 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -7754.0 - Epsilon: 0.41\n",
            "Training...\n",
            "Simulation time: 13.4 s - Training time: 79.3 s - Total: 92.7 s\n",
            "\n",
            "----- Episode 61 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -8225.0 - Epsilon: 0.4\n",
            "Training...\n",
            "Simulation time: 13.5 s - Training time: 79.7 s - Total: 93.2 s\n",
            "\n",
            "----- Episode 62 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -8011.0 - Epsilon: 0.39\n",
            "Training...\n",
            "Simulation time: 13.5 s - Training time: 79.3 s - Total: 92.8 s\n",
            "\n",
            "----- Episode 63 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -8371.0 - Epsilon: 0.38\n",
            "Training...\n",
            "Simulation time: 13.7 s - Training time: 79.5 s - Total: 93.2 s\n",
            "\n",
            "----- Episode 64 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5951.0 - Epsilon: 0.37\n",
            "Training...\n",
            "Simulation time: 13.9 s - Training time: 80.0 s - Total: 93.9 s\n",
            "\n",
            "----- Episode 65 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -7433.0 - Epsilon: 0.36\n",
            "Training...\n",
            "Simulation time: 14.3 s - Training time: 79.8 s - Total: 94.1 s\n",
            "\n",
            "----- Episode 66 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -7869.0 - Epsilon: 0.35\n",
            "Training...\n",
            "Simulation time: 13.9 s - Training time: 79.9 s - Total: 93.8 s\n",
            "\n",
            "----- Episode 67 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -7307.0 - Epsilon: 0.34\n",
            "Training...\n",
            "Simulation time: 14.4 s - Training time: 80.7 s - Total: 95.1 s\n",
            "\n",
            "----- Episode 68 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -7636.0 - Epsilon: 0.33\n",
            "Training...\n",
            "Simulation time: 14.2 s - Training time: 80.0 s - Total: 94.2 s\n",
            "\n",
            "----- Episode 69 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -7565.0 - Epsilon: 0.32\n",
            "Training...\n",
            "Simulation time: 14.6 s - Training time: 79.5 s - Total: 94.1 s\n",
            "\n",
            "----- Episode 70 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -6499.0 - Epsilon: 0.31\n",
            "Training...\n",
            "Simulation time: 14.9 s - Training time: 79.9 s - Total: 94.8 s\n",
            "\n",
            "----- Episode 71 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -8928.0 - Epsilon: 0.3\n",
            "Training...\n",
            "Simulation time: 14.4 s - Training time: 79.1 s - Total: 93.5 s\n",
            "\n",
            "----- Episode 72 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -7098.0 - Epsilon: 0.29\n",
            "Training...\n",
            "Simulation time: 14.3 s - Training time: 79.7 s - Total: 94.0 s\n",
            "\n",
            "----- Episode 73 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -6966.0 - Epsilon: 0.28\n",
            "Training...\n",
            "Simulation time: 14.9 s - Training time: 77.1 s - Total: 92.0 s\n",
            "\n",
            "----- Episode 74 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -7575.0 - Epsilon: 0.27\n",
            "Training...\n",
            "Simulation time: 14.3 s - Training time: 76.7 s - Total: 91.0 s\n",
            "\n",
            "----- Episode 75 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5518.0 - Epsilon: 0.26\n",
            "Training...\n",
            "Simulation time: 15.2 s - Training time: 80.0 s - Total: 95.2 s\n",
            "\n",
            "----- Episode 76 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5699.0 - Epsilon: 0.25\n",
            "Training...\n",
            "Simulation time: 15.4 s - Training time: 80.7 s - Total: 96.1 s\n",
            "\n",
            "----- Episode 77 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -7111.0 - Epsilon: 0.24\n",
            "Training...\n",
            "Simulation time: 15.3 s - Training time: 81.1 s - Total: 96.4 s\n",
            "\n",
            "----- Episode 78 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5835.0 - Epsilon: 0.23\n",
            "Training...\n",
            "Simulation time: 15.3 s - Training time: 80.4 s - Total: 95.7 s\n",
            "\n",
            "----- Episode 79 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5799.0 - Epsilon: 0.22\n",
            "Training...\n",
            "Simulation time: 16.4 s - Training time: 80.2 s - Total: 96.6 s\n",
            "\n",
            "----- Episode 80 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5613.0 - Epsilon: 0.21\n",
            "Training...\n",
            "Simulation time: 16.0 s - Training time: 80.5 s - Total: 96.5 s\n",
            "\n",
            "----- Episode 81 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -6286.0 - Epsilon: 0.2\n",
            "Training...\n",
            "Simulation time: 17.4 s - Training time: 80.4 s - Total: 97.8 s\n",
            "\n",
            "----- Episode 82 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5279.0 - Epsilon: 0.19\n",
            "Training...\n",
            "Simulation time: 15.6 s - Training time: 80.2 s - Total: 95.8 s\n",
            "\n",
            "----- Episode 83 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5432.0 - Epsilon: 0.18\n",
            "Training...\n",
            "Simulation time: 16.3 s - Training time: 80.8 s - Total: 97.1 s\n",
            "\n",
            "----- Episode 84 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -6038.0 - Epsilon: 0.17\n",
            "Training...\n",
            "Simulation time: 16.6 s - Training time: 80.6 s - Total: 97.2 s\n",
            "\n",
            "----- Episode 85 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -6351.0 - Epsilon: 0.16\n",
            "Training...\n",
            "Simulation time: 16.7 s - Training time: 80.4 s - Total: 97.1 s\n",
            "\n",
            "----- Episode 86 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -6157.0 - Epsilon: 0.15\n",
            "Training...\n",
            "Simulation time: 17.3 s - Training time: 80.6 s - Total: 97.9 s\n",
            "\n",
            "----- Episode 87 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5534.0 - Epsilon: 0.14\n",
            "Training...\n",
            "Simulation time: 17.5 s - Training time: 80.8 s - Total: 98.3 s\n",
            "\n",
            "----- Episode 88 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5599.0 - Epsilon: 0.13\n",
            "Training...\n",
            "Simulation time: 17.1 s - Training time: 80.6 s - Total: 97.7 s\n",
            "\n",
            "----- Episode 89 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5271.0 - Epsilon: 0.12\n",
            "Training...\n",
            "Simulation time: 17.3 s - Training time: 80.3 s - Total: 97.6 s\n",
            "\n",
            "----- Episode 90 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5120.0 - Epsilon: 0.11\n",
            "Training...\n",
            "Simulation time: 17.5 s - Training time: 80.9 s - Total: 98.4 s\n",
            "\n",
            "----- Episode 91 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5977.0 - Epsilon: 0.1\n",
            "Training...\n",
            "Simulation time: 16.3 s - Training time: 75.7 s - Total: 92.0 s\n",
            "\n",
            "----- Episode 92 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -6388.0 - Epsilon: 0.09\n",
            "Training...\n",
            "Simulation time: 16.0 s - Training time: 75.9 s - Total: 91.9 s\n",
            "\n",
            "----- Episode 93 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5002.0 - Epsilon: 0.08\n",
            "Training...\n",
            "Simulation time: 16.4 s - Training time: 76.7 s - Total: 93.1 s\n",
            "\n",
            "----- Episode 94 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -4473.0 - Epsilon: 0.07\n",
            "Training...\n",
            "Simulation time: 18.5 s - Training time: 80.4 s - Total: 98.9 s\n",
            "\n",
            "----- Episode 95 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -4363.0 - Epsilon: 0.06\n",
            "Training...\n",
            "Simulation time: 17.9 s - Training time: 80.3 s - Total: 98.2 s\n",
            "\n",
            "----- Episode 96 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -4842.0 - Epsilon: 0.05\n",
            "Training...\n",
            "Simulation time: 17.8 s - Training time: 80.9 s - Total: 98.7 s\n",
            "\n",
            "----- Episode 97 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -4644.0 - Epsilon: 0.04\n",
            "Training...\n",
            "Simulation time: 18.9 s - Training time: 80.8 s - Total: 99.7 s\n",
            "\n",
            "----- Episode 98 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5374.0 - Epsilon: 0.03\n",
            "Training...\n",
            "Simulation time: 18.1 s - Training time: 80.5 s - Total: 98.6 s\n",
            "\n",
            "----- Episode 99 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -5812.0 - Epsilon: 0.02\n",
            "Training...\n",
            "Simulation time: 19.6 s - Training time: 80.6 s - Total: 100.2 s\n",
            "\n",
            "----- Episode 100 of 100\n",
            " Retrying in 1 seconds\n",
            "Loading configuration ... done.\n",
            "Simulating...\n",
            "Total reward: -4301.0 - Epsilon: 0.01\n",
            "Training...\n",
            "Simulation time: 18.2 s - Training time: 81.1 s - Total: 99.3 s\n",
            "\n",
            "----- Start time: 2020-12-08 22:27:23.875469\n",
            "----- End time: 2020-12-09 00:57:28.575941\n",
            "----- Session info saved at: /content/models/model_1/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw71ZZeXDkHu"
      },
      "source": [
        "!rm -fr /content/Deep-QLearning-Agent-for-Traffic-Signal-Control"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EUfxIkC1kPb",
        "outputId": "f579b82c-1dc3-487f-ec48-fb36c745ad3e"
      },
      "source": [
        "!tar -cvf model2.tar /content/models/model_1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar: Removing leading `/' from member names\n",
            "/content/models/model_1/\n",
            "/content/models/model_1/plot_queue_data.txt\n",
            "/content/models/model_1/plot_delay.png\n",
            "/content/models/model_1/plot_reward.png\n",
            "/content/models/model_1/model_structure.png\n",
            "/content/models/model_1/trained_model.h5\n",
            "/content/models/model_1/training_settings.ini\n",
            "/content/models/model_1/plot_queue.png\n",
            "/content/models/model_1/plot_reward_data.txt\n",
            "/content/models/model_1/plot_delay_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6FI_Lb2113c"
      },
      "source": [
        "!gzip /content/model2.tar"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Rv0dGgl6bSrE",
        "outputId": "7b0b5f80-5329-4c10-89f0-52cfcc55acea"
      },
      "source": [
        "from google.colab import files\n",
        "files.download( \"/content/model2.tar.gz\" ) "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3707b1ce-fd99-4ead-bc64-04462393b2c3\", \"model2.tar.gz\", 5355770)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoyQMaWXkyu1"
      },
      "source": [
        "Check the */models/* directory for output files"
      ]
    }
  ]
}